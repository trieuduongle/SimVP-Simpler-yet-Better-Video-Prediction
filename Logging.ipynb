{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trieuduongle/SimVP-Simpler-yet-Better-Video-Prediction/blob/logging/Logging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLrjGVvQWNxH",
        "outputId": "07bf787d-a1ec-43d2-bc8a-952ac8f21a70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/trieuduongle/SimVP-Simpler-yet-Better-Video-Prediction.git '/content/drive/My Drive/Duong/Logging_SimVP'"
      ],
      "metadata": {
        "id": "OC8ei095vJOG",
        "outputId": "ad0be6e0-380e-488b-e0f1-8583be844687",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '/content/drive/My Drive/Duong/Logging_SimVP'...\n",
            "remote: Enumerating objects: 456, done.\u001b[K\n",
            "remote: Counting objects: 100% (388/388), done.\u001b[K\n",
            "remote: Compressing objects: 100% (176/176), done.\u001b[K\n",
            "remote: Total 456 (delta 251), reused 339 (delta 209), pack-reused 68\u001b[K\n",
            "Receiving objects: 100% (456/456), 207.45 KiB | 4.61 MiB/s, done.\n",
            "Resolving deltas: 100% (284/284), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snonURDKWpwk",
        "outputId": "ff549675-afd9-4a0b-a3f2-5f58c1143a0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Duong/Logging_SimVP\n"
          ]
        }
      ],
      "source": [
        "%cd '/content/drive/My Drive/Duong/Logging_SimVP'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvsZ3mzo9BxB"
      },
      "source": [
        "#Trainning with Moving MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git checkout logging"
      ],
      "metadata": {
        "id": "6_bG94-6vO4d",
        "outputId": "e36e8cb5-a511-44c5-b2e9-56d06a949da4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Branch 'logging' set up to track remote branch 'logging' from 'origin'.\n",
            "Switched to a new branch 'logging'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmpXMxrMC50q",
        "outputId": "d9763482-0277-4a40-8507-e417b4cc2a26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects:  33% (1/3)\u001b[K\rremote: Counting objects:  66% (2/3)\u001b[K\rremote: Counting objects: 100% (3/3)\u001b[K\rremote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 3 (delta 2), reused 3 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects:  33% (1/3)   \rUnpacking objects:  66% (2/3)   \rUnpacking objects: 100% (3/3)   \rUnpacking objects: 100% (3/3), done.\n",
            "From https://github.com/trieuduongle/SimVP-Simpler-yet-Better-Video-Prediction\n",
            "   a148857..e97f3cb  logging    -> origin/logging\n",
            "Updating a148857..e97f3cb\n",
            "Fast-forward\n",
            " exp.py | 2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " 1 file changed, 1 insertion(+), 1 deletion(-)\n"
          ]
        }
      ],
      "source": [
        "!git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KqQEKFi0jG9",
        "outputId": "165de2e0-b174-49f6-a903-10ff4f12cd4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: hickle==5.0.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 1)) (5.0.2)\n",
            "Requirement already satisfied: tqdm==4.64.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 2)) (4.64.1)\n",
            "Requirement already satisfied: scikit-image==0.19.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 3)) (0.19.3)\n",
            "Requirement already satisfied: tensorboardX==2.5.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 4)) (2.5.1)\n",
            "Requirement already satisfied: h5py>=2.10.0 in /usr/local/lib/python3.8/dist-packages (from hickle==5.0.2->-r requirements.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: numpy!=1.20,>=1.8 in /usr/local/lib/python3.8/dist-packages (from hickle==5.0.2->-r requirements.txt (line 1)) (1.21.6)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image==0.19.3->-r requirements.txt (line 3)) (7.1.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image==0.19.3->-r requirements.txt (line 3)) (1.7.3)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image==0.19.3->-r requirements.txt (line 3)) (2022.10.10)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image==0.19.3->-r requirements.txt (line 3)) (21.3)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.8/dist-packages (from scikit-image==0.19.3->-r requirements.txt (line 3)) (2.8.8)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image==0.19.3->-r requirements.txt (line 3)) (1.4.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image==0.19.3->-r requirements.txt (line 3)) (2.9.0)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX==2.5.1->-r requirements.txt (line 4)) (3.19.6)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->scikit-image==0.19.3->-r requirements.txt (line 3)) (3.0.9)\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJ-upBDx71pO",
        "outputId": "66472ff5-0288-444f-abc8-e1f88b404fb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects:  20% (1/5)\u001b[K\rremote: Counting objects:  40% (2/5)\u001b[K\rremote: Counting objects:  60% (3/5)\u001b[K\rremote: Counting objects:  80% (4/5)\u001b[K\rremote: Counting objects: 100% (5/5)\u001b[K\rremote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1)\u001b[K\rremote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 3 (delta 2), reused 3 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects:  33% (1/3)   \rUnpacking objects:  66% (2/3)   \rUnpacking objects: 100% (3/3)   \rUnpacking objects: 100% (3/3), done.\n",
            "From https://github.com/trieuduongle/SimVP-Simpler-yet-Better-Video-Prediction\n",
            "   126d887..74aa152  logging    -> origin/logging\n",
            "Updating 126d887..74aa152\n",
            "Fast-forward\n",
            " exp.py | 4 \u001b[32m++\u001b[m\u001b[31m--\u001b[m\n",
            " 1 file changed, 2 insertions(+), 2 deletions(-)\n",
            "Use GPU: 0\n",
            "\n",
            "device: \tcuda\t\n",
            "res_dir: \t./results/mnist/20230101\t\n",
            "ex_name: \tDebug\t\n",
            "use_gpu: \tTrue\t\n",
            "gpu: \t0\t\n",
            "seed: \t1\t\n",
            "batch_size: \t64\t\n",
            "val_batch_size: \t64\t\n",
            "data_root: \t/content/drive/My Drive/Duong/datasets/\t\n",
            "dataname: \tmmnist\t\n",
            "num_workers: \t8\t\n",
            "in_shape: \t[10, 1, 64, 64]\t\n",
            "hid_S: \t64\t\n",
            "hid_T: \t256\t\n",
            "N_S: \t4\t\n",
            "N_T: \t8\t\n",
            "groups: \t4\t\n",
            "epochs: \t5\t\n",
            "log_step: \t1\t\n",
            "save_epoch_freq: \t1\t\n",
            "lr: \t0.01\t\n",
            "resume_path: \t\t\n",
            "sample_path: \t./samples\t\n",
            "sample_epoch: \t1\t\n",
            "pre_seq_length: \t10\t\n",
            "aft_seq_length: \t10\t\n",
            "log_path: \t./logs\t\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>  start <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "train loss: 0.7362:   1% 1/157 [00:08<22:21,  8.60s/it]\n",
            "vali loss: 0.1207:  10% 16/157 [00:13<01:57,  1.20it/s]\n",
            "vali mse:497.9384, mae:979.9408, ssim:0.1604, psnr:33.2504\n",
            "Epoch: 1 | Train Loss: 0.4230 Vali Loss: 0.1216 | Take 28.2152 seconds\n",
            "\n",
            "Validation loss decreased (inf --> 0.121567).  Saving model ...\n",
            "torch.Size([10, 1, 64, 64])\n",
            "torch.Size([10, 1, 64, 64])\n",
            "torch.Size([3, 68, 662])\n",
            "train loss: 0.1452:   1% 1/157 [00:06<15:41,  6.04s/it]\n",
            "vali loss: 0.1835:  10% 16/157 [00:13<02:02,  1.15it/s]\n",
            "vali mse:749.0175, mae:1645.3269, ssim:0.0064, psnr:27.8388\n",
            "Epoch: 2 | Train Loss: 0.1329 Vali Loss: 0.1829 | Take 25.5043 seconds\n",
            "\n",
            "torch.Size([10, 1, 64, 64])\n",
            "torch.Size([10, 1, 64, 64])\n",
            "torch.Size([3, 68, 662])\n",
            "train loss: 0.1321:   1% 1/157 [00:06<16:03,  6.18s/it]\n",
            "vali loss: 0.0824:  10% 16/157 [00:14<02:09,  1.09it/s]\n",
            "vali mse:337.8947, mae:980.7122, ssim:0.0073, psnr:27.9116\n",
            "Epoch: 3 | Train Loss: 0.1582 Vali Loss: 0.0825 | Take 26.4194 seconds\n",
            "\n",
            "Validation loss decreased (0.121567 --> 0.082494).  Saving model ...\n",
            "torch.Size([10, 1, 64, 64])\n",
            "torch.Size([10, 1, 64, 64])\n",
            "torch.Size([3, 68, 662])\n",
            "train loss: 0.0599:   1% 1/157 [00:05<15:34,  5.99s/it]\n",
            "vali loss: 0.0560:  10% 16/157 [00:14<02:04,  1.14it/s]\n",
            "vali mse:232.0825, mae:564.0913, ssim:0.0738, psnr:29.6941\n",
            "Epoch: 4 | Train Loss: 0.0708 Vali Loss: 0.0567 | Take 25.5958 seconds\n",
            "\n",
            "Validation loss decreased (0.082494 --> 0.056661).  Saving model ...\n",
            "torch.Size([10, 1, 64, 64])\n",
            "torch.Size([10, 1, 64, 64])\n",
            "torch.Size([3, 68, 662])\n",
            "train loss: 0.0605:   1% 1/157 [00:06<15:44,  6.05s/it]\n",
            "vali loss: 0.0623:  10% 16/157 [00:14<01:55,  1.22it/s]Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "vali loss: 0.0623:  10% 16/157 [00:14<02:06,  1.12it/s]\n",
            "vali mse:258.9950, mae:531.3178, ssim:0.7298, psnr:37.0295\n",
            "Epoch: 5 | Train Loss: 0.0585 Vali Loss: 0.0632 | Take 26.0646 seconds\n",
            "\n",
            "torch.Size([10, 1, 64, 64])\n",
            "torch.Size([10, 1, 64, 64])\n",
            "torch.Size([3, 68, 662])\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>> testing <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 55, in <module>\n",
            "    mse = exp.test(args)\n",
            "  File \"/content/drive/MyDrive/Duong/Logging_SimVP/exp.py\", line 215, in test\n",
            "    pred_y = self.model(batch_x.to(self.device))\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/drive/MyDrive/Duong/Logging_SimVP/model.py\", line 100, in forward\n",
            "    hid = self.hid(z)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/drive/MyDrive/Duong/Logging_SimVP/model.py\", line 77, in forward\n",
            "    z = self.dec[i](torch.cat([z, skips[-i]], dim=1))\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/drive/MyDrive/Duong/Logging_SimVP/modules.py\", line 65, in forward\n",
            "    y += layer(x)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/drive/MyDrive/Duong/Logging_SimVP/modules.py\", line 46, in forward\n",
            "    y = self.conv(x)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\", line 463, in forward\n",
            "    return self._conv_forward(input, self.weight, self.bias)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\", line 459, in _conv_forward\n",
            "    return F.conv2d(input, weight, bias, self.stride,\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 14.76 GiB total capacity; 13.39 GiB already allocated; 3.75 MiB free; 13.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
          ]
        }
      ],
      "source": [
        "!python main.py \\\n",
        "  --res_dir './results/mnist/20230101' \\\n",
        "  --data_root '/content/drive/My Drive/Duong/datasets/' \\\n",
        "  --dataname 'mmnist' \\\n",
        "  --num_workers 8 \\\n",
        "  --batch_size 64 \\\n",
        "  --val_batch_size 64 \\\n",
        "  --epochs 5 \\\n",
        "  --save_epoch_freq 1 \\\n",
        "  --sample_epoch 1 \\\n",
        "  --in_shape 10 1 64 64 \\\n",
        "  --pre_seq_length 10 \\\n",
        "  --aft_seq_length 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73U9z7Fazp94"
      },
      "source": [
        "# Traning with KTH dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmo39p-uyn2h",
        "outputId": "7e5fb337-a81b-4f93-bdbb-f38ff0405419"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/Duong/datasets/kth.zip\n",
            "warning:  stripped absolute path spec from /\n",
            "mapname:  conversion of  failed\n",
            " extracting: /content/drive/My Drive/Duong/datasets/kth/dataloader_kth.py  \n",
            " extracting: /content/drive/My Drive/Duong/datasets/kth/test_data_gzip.hkl  \n",
            " extracting: /content/drive/My Drive/Duong/datasets/kth/train_data_gzip.hkl  \n",
            " extracting: /content/drive/My Drive/Duong/datasets/kth/test_indices_gzip.hkl  \n",
            " extracting: /content/drive/My Drive/Duong/datasets/kth/train_indices_gzip.hkl  \n"
          ]
        }
      ],
      "source": [
        "!unzip '/content/drive/My Drive/Duong/datasets/kth.zip' -d '/content/drive/My Drive/Duong/datasets/kth'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5gsFHkQ2lRC",
        "outputId": "7aed2756-6e3f-4541-8c3d-059f340357ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting hickle\n",
            "  Downloading hickle-5.0.2-py3-none-any.whl (107 kB)\n",
            "\u001b[K     |████████████████████████████████| 107 kB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy!=1.20,>=1.8 in /usr/local/lib/python3.8/dist-packages (from hickle) (1.21.6)\n",
            "Requirement already satisfied: h5py>=2.10.0 in /usr/local/lib/python3.8/dist-packages (from hickle) (3.1.0)\n",
            "Installing collected packages: hickle\n",
            "Successfully installed hickle-5.0.2\n"
          ]
        }
      ],
      "source": [
        "!pip install hickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdLLvosiztz6",
        "outputId": "016df212-be82-44c1-d51c-480c584ec023"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Use GPU: 0\n",
            "exist\n",
            "tcmalloc: large alloc 16681664512 bytes == 0x6478000 @  0x7f8fcffde1e7 0x7f8f5ffd414e 0x7f8f6002c745 0x7f8f5ffd79c8 0x5d7f65 0x560200 0x55e571 0x4eae40 0x7f8f2ea70f05 0x5d813c 0x4ff485 0x46eb22 0x59daf9 0x5176d9 0x55fa11 0x5d7c18 0x49ec69 0x5d7c18 0x49ec69 0x55e571 0x5d7cf1 0x49caa1 0x5d7c18 0x49ca7c 0x55e571 0x5d7cf1 0x49ec69 0x55e571 0x5d7cf1 0x5d77c6 0x561051\n",
            "tcmalloc: large alloc 16681664512 bytes == 0x3fc394000 @  0x7f8fcffde1e7 0x7f8f5ffd414e 0x7f8f60030166 0x7f8f60022665 0x7f8f600d3640 0x5aa114 0x49ced5 0x5d7c18 0x49ec69 0x5d7c18 0x49ec69 0x55e571 0x5d7cf1 0x49caa1 0x5d7c18 0x49ca7c 0x55e571 0x5d7cf1 0x49ec69 0x55e571 0x5d7cf1 0x5d77c6 0x561051 0x5d7c18 0x49ca7c 0x5d7c18 0x49ca7c 0x55e858 0x5d7cf1 0x5d9412 0x586306\n",
            "tcmalloc: large alloc 9808510976 bytes == 0x63f6000 @  0x7f8fcffde1e7 0x7f8f5ffd414e 0x7f8f6002c745 0x7f8f5ffd79c8 0x5d7f65 0x560200 0x55e571 0x4eae40 0x7f8f2ea70f05 0x5d813c 0x4ff485 0x46eb22 0x59daf9 0x5176d9 0x55fa11 0x5d7c18 0x49ec69 0x5d7c18 0x49ec69 0x55e571 0x5d7cf1 0x49caa1 0x5d7c18 0x49ca7c 0x55e571 0x5d7cf1 0x49ec69 0x55e571 0x5d7cf1 0x5d77c6 0x561051\n",
            "tcmalloc: large alloc 9808510976 bytes == 0x7dfab6000 @  0x7f8fcffde1e7 0x7f8f5ffd414e 0x7f8f60030166 0x7f8f60022665 0x7f8f600d3640 0x5aa114 0x49ced5 0x5d7c18 0x49ec69 0x5d7c18 0x49ec69 0x55e571 0x5d7cf1 0x49caa1 0x5d7c18 0x49ca7c 0x55e571 0x5d7cf1 0x49ec69 0x55e571 0x5d7cf1 0x5d77c6 0x561051 0x5d7c18 0x49ca7c 0x5d7c18 0x49ca7c 0x55e858 0x5d7cf1 0x5d9412 0x586306\n",
            "\n",
            "device: \tcuda\t\n",
            "res_dir: \t./results/20221203\t\n",
            "ex_name: \tDebug\t\n",
            "use_gpu: \tTrue\t\n",
            "gpu: \t0\t\n",
            "seed: \t1\t\n",
            "batch_size: \t8\t\n",
            "val_batch_size: \t16\t\n",
            "data_root: \t/content/drive/My Drive/Duong/datasets/kth/\t\n",
            "dataname: \tkth\t\n",
            "num_workers: \t1\t\n",
            "in_shape: \t[10, 1, 128, 128]\t\n",
            "hid_S: \t64\t\n",
            "hid_T: \t256\t\n",
            "N_S: \t4\t\n",
            "N_T: \t8\t\n",
            "groups: \t4\t\n",
            "epochs: \t51\t\n",
            "log_step: \t1\t\n",
            "save_epoch_freq: \t10\t\n",
            "lr: \t0.01\t\n",
            "exist\n",
            "tcmalloc: large alloc 16681664512 bytes == 0x7f8acbb20000 @  0x7f8fcffde1e7 0x7f8f5ffd414e 0x7f8f6002c745 0x7f8f5ffd79c8 0x5d7f65 0x560200 0x55e571 0x4eae40 0x7f8f2ea70f05 0x5d813c 0x4ff485 0x46eb22 0x59daf9 0x5176d9 0x55fa11 0x5d7c18 0x49ec69 0x5d7c18 0x49ec69 0x55e571 0x5d7cf1 0x49caa1 0x5d7c18 0x49ca7c 0x55e571 0x5d7cf1 0x49ec69 0x55e571 0x5d7cf1 0x5d77c6 0x561051\n",
            "tcmalloc: large alloc 16681664512 bytes == 0x7f86e9640000 @  0x7f8fcffde1e7 0x7f8f5ffd414e 0x7f8f60030166 0x7f8f60022665 0x7f8f600d3640 0x5aa114 0x49ced5 0x5d7c18 0x49ec69 0x5d7c18 0x49ec69 0x55e571 0x5d7cf1 0x49caa1 0x5d7c18 0x49ca7c 0x55e571 0x5d7cf1 0x49ec69 0x55e571 0x5d7cf1 0x5d77c6 0x561051 0x5d7c18 0x49ca7c 0x55e858 0x5d7cf1 0x5d9412 0x586306 0x5d808f 0x55f3fd\n",
            "tcmalloc: large alloc 9808510976 bytes == 0x539d4000 @  0x7f8fcffde1e7 0x7f8f5ffd414e 0x7f8f6002c745 0x7f8f5ffd79c8 0x5d7f65 0x560200 0x55e571 0x4eae40 0x7f8f2ea70f05 0x5d813c 0x4ff485 0x46eb22 0x59daf9 0x5176d9 0x55fa11 0x5d7c18 0x49ec69 0x5d7c18 0x49ec69 0x55e571 0x5d7cf1 0x49caa1 0x5d7c18 0x49ca7c 0x55e571 0x5d7cf1 0x49ec69 0x55e571 0x5d7cf1 0x5d77c6 0x561051\n",
            "tcmalloc: large alloc 9808510976 bytes == 0x7f8acbb20000 @  0x7f8fcffde1e7 0x7f8f5ffd414e 0x7f8f60030166 0x7f8f60022665 0x7f8f600d3640 0x5aa114 0x49ced5 0x5d7c18 0x49ec69 0x5d7c18 0x49ec69 0x55e571 0x5d7cf1 0x49caa1 0x5d7c18 0x49ca7c 0x55e571 0x5d7cf1 0x49ec69 0x55e571 0x5d7cf1 0x5d77c6 0x561051 0x5d7c18 0x49ca7c 0x55e858 0x5d7cf1 0x5d9412 0x586306 0x5d808f 0x55f3fd\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>  start <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "  0% 0/650 [00:08<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 46, in <module>\n",
            "    exp.train(args)\n",
            "  File \"/content/drive/MyDrive/Duong/SimVP/code/exp.py\", line 104, in train\n",
            "    loss = self.criterion(pred_y, batch_y)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/loss.py\", line 530, in forward\n",
            "    return F.mse_loss(input, target, reduction=self.reduction)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\", line 3279, in mse_loss\n",
            "    expanded_input, expanded_target = torch.broadcast_tensors(input, target)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/functional.py\", line 73, in broadcast_tensors\n",
            "    return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]\n",
            "RuntimeError: The size of tensor a (10) must match the size of tensor b (20) at non-singleton dimension 1\n"
          ]
        }
      ],
      "source": [
        "!python main.py \\\n",
        "  --res_dir './results/20221203' \\\n",
        "  --data_root '/content/drive/My Drive/Duong/datasets/kth/' \\\n",
        "  --dataname 'kth' \\\n",
        "  --num_workers 1 \\\n",
        "  --batch_size 8 \\\n",
        "  --in_shape 10 1 128 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ii2kRUcy3-0h",
        "outputId": "03bab3be-ad6c-4a25-ee7c-cc8c71ffed74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataloader_kth.py   test_indices_gzip.hkl  train_indices_gzip.hkl\n",
            "test_data_gzip.hkl  train_data_gzip.hkl\n"
          ]
        }
      ],
      "source": [
        "!ls '/content/drive/My Drive/Duong/datasets/kth'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GZX1vhNsq0D"
      },
      "source": [
        "#Try to run model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsQ_SKEUst1A",
        "outputId": "a1ae6996-5aac-4eac-eb4a-8afa1c33e790"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "remote: Enumerating objects: 51, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/51)\u001b[K\rremote: Counting objects:   3% (2/51)\u001b[K\rremote: Counting objects:   5% (3/51)\u001b[K\rremote: Counting objects:   7% (4/51)\u001b[K\rremote: Counting objects:   9% (5/51)\u001b[K\rremote: Counting objects:  11% (6/51)\u001b[K\rremote: Counting objects:  13% (7/51)\u001b[K\rremote: Counting objects:  15% (8/51)\u001b[K\rremote: Counting objects:  17% (9/51)\u001b[K\rremote: Counting objects:  19% (10/51)\u001b[K\rremote: Counting objects:  21% (11/51)\u001b[K\rremote: Counting objects:  23% (12/51)\u001b[K\rremote: Counting objects:  25% (13/51)\u001b[K\rremote: Counting objects:  27% (14/51)\u001b[K\rremote: Counting objects:  29% (15/51)\u001b[K\rremote: Counting objects:  31% (16/51)\u001b[K\rremote: Counting objects:  33% (17/51)\u001b[K\rremote: Counting objects:  35% (18/51)\u001b[K\rremote: Counting objects:  37% (19/51)\u001b[K\rremote: Counting objects:  39% (20/51)\u001b[K\rremote: Counting objects:  41% (21/51)\u001b[K\rremote: Counting objects:  43% (22/51)\u001b[K\rremote: Counting objects:  45% (23/51)\u001b[K\rremote: Counting objects:  47% (24/51)\u001b[K\rremote: Counting objects:  49% (25/51)\u001b[K\rremote: Counting objects:  50% (26/51)\u001b[K\rremote: Counting objects:  52% (27/51)\u001b[K\rremote: Counting objects:  54% (28/51)\u001b[K\rremote: Counting objects:  56% (29/51)\u001b[K\rremote: Counting objects:  58% (30/51)\u001b[K\rremote: Counting objects:  60% (31/51)\u001b[K\rremote: Counting objects:  62% (32/51)\u001b[K\rremote: Counting objects:  64% (33/51)\u001b[K\rremote: Counting objects:  66% (34/51)\u001b[K\rremote: Counting objects:  68% (35/51)\u001b[K\rremote: Counting objects:  70% (36/51)\u001b[K\rremote: Counting objects:  72% (37/51)\u001b[K\rremote: Counting objects:  74% (38/51)\u001b[K\rremote: Counting objects:  76% (39/51)\u001b[K\rremote: Counting objects:  78% (40/51)\u001b[K\rremote: Counting objects:  80% (41/51)\u001b[K\rremote: Counting objects:  82% (42/51)\u001b[K\rremote: Counting objects:  84% (43/51)\u001b[K\rremote: Counting objects:  86% (44/51)\u001b[K\rremote: Counting objects:  88% (45/51)\u001b[K\rremote: Counting objects:  90% (46/51)\u001b[K\rremote: Counting objects:  92% (47/51)\u001b[K\rremote: Counting objects:  94% (48/51)\u001b[K\rremote: Counting objects:  96% (49/51)\u001b[K\rremote: Counting objects:  98% (50/51)\u001b[K\rremote: Counting objects: 100% (51/51)\u001b[K\rremote: Counting objects: 100% (51/51), done.\u001b[K\n",
            "remote: Compressing objects:   3% (1/28)\u001b[K\rremote: Compressing objects:   7% (2/28)\u001b[K\rremote: Compressing objects:  10% (3/28)\u001b[K\rremote: Compressing objects:  14% (4/28)\u001b[K\rremote: Compressing objects:  17% (5/28)\u001b[K\rremote: Compressing objects:  21% (6/28)\u001b[K\rremote: Compressing objects:  25% (7/28)\u001b[K\rremote: Compressing objects:  28% (8/28)\u001b[K\rremote: Compressing objects:  32% (9/28)\u001b[K\rremote: Compressing objects:  35% (10/28)\u001b[K\rremote: Compressing objects:  39% (11/28)\u001b[K\rremote: Compressing objects:  42% (12/28)\u001b[K\rremote: Compressing objects:  46% (13/28)\u001b[K\rremote: Compressing objects:  50% (14/28)\u001b[K\rremote: Compressing objects:  53% (15/28)\u001b[K\rremote: Compressing objects:  57% (16/28)\u001b[K\rremote: Compressing objects:  60% (17/28)\u001b[K\rremote: Compressing objects:  64% (18/28)\u001b[K\rremote: Compressing objects:  67% (19/28)\u001b[K\rremote: Compressing objects:  71% (20/28)\u001b[K\rremote: Compressing objects:  75% (21/28)\u001b[K\rremote: Compressing objects:  78% (22/28)\u001b[K\rremote: Compressing objects:  82% (23/28)\u001b[K\rremote: Compressing objects:  85% (24/28)\u001b[K\rremote: Compressing objects:  89% (25/28)\u001b[K\rremote: Compressing objects:  92% (26/28)\u001b[K\rremote: Compressing objects:  96% (27/28)\u001b[K\rremote: Compressing objects: 100% (28/28)\u001b[K\rremote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 41 (delta 27), reused 22 (delta 12), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (41/41), done.\n",
            "From https://github.com/trieuduongle/SimVP-Simpler-yet-Better-Video-Prediction\n",
            "   f07a3d2..36c3530  colab      -> origin/colab\n",
            "Updating f07a3d2..36c3530\n",
            "Fast-forward\n",
            " API/dataloader.py              |   9 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " API/dataloader_kth.py          |   4 \u001b[32m+\u001b[m\n",
            " API/dataloader_moving_mnist.py |   4 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " SimVP.ipynb                    | 665 \u001b[32m+++++++++++++++++++++++++++++++++++++++\u001b[m\u001b[31m--\u001b[m\n",
            " create_parser.py               |  45 \u001b[32m+++\u001b[m\n",
            " exp.py                         |  10 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " fake_training.py               | 131 \u001b[32m++++++++\u001b[m\n",
            " interpolate.py                 |   9 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " main.py                        |   2 \u001b[32m+\u001b[m\n",
            " requirements.txt               |   4 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " 10 files changed, 847 insertions(+), 36 deletions(-)\n",
            " create mode 100644 create_parser.py\n",
            " create mode 100644 fake_training.py\n"
          ]
        }
      ],
      "source": [
        "!git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDf17M5msvnM",
        "outputId": "8899df8b-92f6-49d9-e7cd-6d6a984278e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Use GPU: 0\n",
            "\n",
            "device: \tcuda\t\n",
            "res_dir: \t./results/fake_training/\t\n",
            "ex_name: \tDebug\t\n",
            "use_gpu: \tTrue\t\n",
            "gpu: \t0\t\n",
            "seed: \t1\t\n",
            "batch_size: \t16\t\n",
            "val_batch_size: \t16\t\n",
            "data_root: \t/content/drive/My Drive/Duong/datasets/\t\n",
            "dataname: \tmmnist\t\n",
            "num_workers: \t8\t\n",
            "in_shape: \t[10, 1, 64, 64]\t\n",
            "hid_S: \t64\t\n",
            "hid_T: \t256\t\n",
            "N_S: \t4\t\n",
            "N_T: \t8\t\n",
            "groups: \t4\t\n",
            "epochs: \t51\t\n",
            "log_step: \t1\t\n",
            "save_epoch_freq: \t2\t\n",
            "lr: \t0.01\t\n",
            "resume_path: \t\t\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>  start <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "  0% 0/625 [00:00<?, ?it/s]longger\n",
            "train loss: 0.1219:   0% 1/625 [00:05<1:00:43,  5.84s/it]longger\n",
            "train loss: 0.5743:   0% 2/625 [00:07<32:35,  3.14s/it]  longger\n",
            "train loss: 0.1668:   0% 3/625 [00:08<23:36,  2.28s/it]longger\n",
            "train loss: 0.0751:   1% 4/625 [00:09<19:24,  1.88s/it]longger\n",
            "train loss: 0.1162:   1% 5/625 [00:10<17:05,  1.65s/it]longger\n",
            "train loss: 0.1168:   1% 6/625 [00:12<15:41,  1.52s/it]longger\n",
            "train loss: 0.0851:   1% 7/625 [00:13<14:48,  1.44s/it]longger\n",
            "train loss: 0.0640:   1% 8/625 [00:14<14:14,  1.38s/it]longger\n",
            "train loss: 0.0572:   1% 9/625 [00:15<13:51,  1.35s/it]longger\n",
            "train loss: 0.0524:   2% 10/625 [00:17<13:36,  1.33s/it]longger\n",
            "train loss: 0.0566:   2% 10/625 [00:18<19:00,  1.86s/it]\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 47, in <module>\n",
            "    exp.train(args)\n",
            "  File \"/content/drive/MyDrive/Duong/SimVP/code/exp.py\", line 137, in train\n",
            "    self.optimizer.step()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py\", line 65, in wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py\", line 113, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/optim/adam.py\", line 157, in step\n",
            "    adam(params_with_grad,\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/optim/adam.py\", line 213, in adam\n",
            "    func(params,\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/optim/adam.py\", line 305, in _single_tensor_adam\n",
            "    denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python main.py \\\n",
        "  --res_dir './results/fake_training/' \\\n",
        "  --data_root '/content/drive/My Drive/Duong/datasets/' \\\n",
        "  --dataname 'mmnist' \\\n",
        "  --num_workers 8 \\\n",
        "  --batch_size 16 \\\n",
        "  --save_epoch_freq 2\\\n",
        "  --in_shape 10 1 64 64"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}